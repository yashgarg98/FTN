<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Factorized Tensor Networks for Multi-Task and Multi-Domain Learning">
  <meta property="og:title" content="Multi-task learning"/>
  <meta property="og:description" content="Factorized Tensor Networks for Multi-Task and Multi-Domain Learning"/>
  <!-- <meta property="og:url" content="URL OF THE WEBSITE"/> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="computational imaging, machine learning, domain adaptation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Domain Expansion via Network Adaptation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Domain Expansion via Network Adaptation for Solving Inverse Problems</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="mailto: nyism001@ucr.edu" target="_blank">Nebiyou Yismaw</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://engineering.wustl.edu/faculty/Ulugbek-Kamilov.html" target="_blank">Ulugbek S. Kamilov</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://intra.ece.ucr.edu/~sasif/" target="_blank">M. Salman Asif</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <small>1. University of California Riverside</small>
                  </span><br>
                    <span class="author-block">
                      <small>2. Washington University in St. Louis</small>
                    </span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2310.06235" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2310.06235" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
       <center>
      <img src="static/images/intro_images_updated_all_v4.png" width="850" />
       </center>
      <h4 style="font-size: 16px;" class="subtitle has-text-justified">

        Fig. 1: Artifact removal (AR) networks trained on MRI scans (fastMRI AR) and face images (celebA AR) suffer from performance degradation under domain shifts, resulting in poor reconstruction quality (as indicated by PSNR and SSIM values under each image). Our proposed network (Modulated AR) adapts fastMRI AR for face image reconstruction by learning rank-one factors (modulations). The network stores shared and domain-specific modulations separately. During inference, it applies the correct modulation according to the specified domain. Our proposed network retains the performance of fastMRI AR on MR images and achieves competitive reconstruction quality with celebA AR on face images.
      </h4>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Deep learning-based methods deliver state-of-the-art performance for solving inverse problems that arise in computational imaging. These methods can be broadly divided into two groups: (1) learn a network to map  measurements to the signal estimate, which is known to be fragile; (2) learn a prior for the signal to use in an optimization-based recovery. Despite the impressive results from the latter approach, many of these methods also lack robustness to shifts in data distribution, measurements, and noise levels. Such domain shifts result in a performance gap and in some cases introduce undesired artifacts in the estimated signal. In this paper, we explore the qualitative and quantitative effects of various domain shifts and propose a flexible and parameter efficient framework that  adapt pretrained networks to such shifts. We demonstrate the effectiveness of our method for a number of natural image, MRI, and CT reconstructions tasks under domain, measurement model, and noise-level shifts. Our experiments demonstrate that our method provides significantly better performance and parameter efficiency compared to existing domain adaptation techniques.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Intro -->
<section class="section is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"> Impact of Domain Shift</h2>

          <div class="hero-body">
             <center>
            <img src="static/images/intro_images_updated_all_v3.png" width="550" />
             </center>
             <div class="subtitle has-text-justified">
            <h4 style="font-size: 16px;" class="subtitle has-text-justified">
              Fig. 2: Examples of image reconstruction under domain shifts. We use two artifact removal (AR) network trained for fastMRI and celebA images in deep unrolled framework. Second and third columns show both images reconstructed with fastMRI AR and celebA AR, respectively. Reconstruction quality degrades with domain shifts (PSNR and SSIM reported under each image). Our proposed network adaptation method, where we adapt celebA AR to recover an MR image (top row) and fastMRI AR to recover a celebA image (bottom row).
            </h4>
            </div>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <!-- <hr> -->
<!-- Intro -->
<section class="section hero is-light">

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Learning rank-1 factors for domain adaptation</h2>

          <div class="hero-body">
             <center>
            <img src="static/images/prop_multi_domain_net_arch_v2.png" width="600" />
             </center>
             <div style="font-size: 16px;" class="subtitle has-text-justified">
               <!-- <h4 class=""> -->
                 Fig. 3: Overview of our factorized network that uses modulated convolutions for domain adaptation. Our network follows the DNCNN architecture that leverages modulated convolution for domain adaptation. After trained on the source domain, the network learns low-rank modulations for each domain while keeping the base network parameters frozen. Using a domain identifier, the network selects the appropriate low-rank factors during inference and applies them to the pretrained network through element-wise multiplication.
               <!-- </h4> -->
            </div>

          </div>

      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <center>
  <div class="hero-body">

    <div class="container center">
        <h2 class="title is-3">Domain, forward model, and noise level adaptation</h2>
        <!-- <div class="content has-text-justified"> -->
        <p class="has-text-justified" style="width: 900px;">
          We performed a number of experiments to analyze the effects of shifts in different parts of the inverse problem.
          The shifts can occur in the data distribution \( \mathbf{x} \), the forward model \( \mathbf{A} \), and the measurement
          noise \( \eta \). we start with a fixed base network, which we refer to as Base AR, and learn domain-specific rank-one modulations.
          Base AR is trained to reconstruct MR images from \( 4\times\) radially sub-sampled Fourier measurements without any measurement noise.
        </p>
        <!-- <div> -->
      <div id="results-carousel" class="carousel results-carousel">

       <div class="item" width="300px">
         <h2 class="title is-4">Domain shift adaptation</h2>
        <!-- Your image here -->
        <img src="static/images/domain_shift_cropped_updated_v2.png" alt="MY ALT TEXT" width="600" />
        <h2 style="font-size: 16px; max-width: 600px;" class="subtitle has-text-justified">
          Fig. 4: Sample ground truth images in the first column and reconstruction of these images using three AR networks trained on Face, MR, and CT images in the subsequent three columns .
          Our modulated AR, shown in the last column effectively removes this artifacts and closes the performance gap.
        </h2>
      </div>
      <div class="item">
        <h2 class="title is-4">Sampling pattern adaptation</h2>
        <!-- Your image here -->
        <img src="static/images/sampling_pattern_shift.png" alt="MY ALT TEXT" width="600" />
        <h2 style="font-size: 16px; max-width: 600px;" class="subtitle has-text-justified">
          Fig. 5:  Reconstruction results under sampling pattern shifts. AR trained on radial pattern performs poorly when tested on Cartesian sampled patterns. Our Modulated AR applies low-rank modulations to adapt Radial AR to Cartesian samples.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="title is-4">Sampling ratio adaptation</h2>
        <img src="static/images/sampling_ratio_shift_all.png" alt="MY ALT TEXT" width="600" />
        <h2 style="font-size: 16px; max-width: 600px;" class="subtitle has-text-justified">
         Fig. 6:  Examples of image reconstruction under sampling ratio shifts. Our Modulated AR shows an average superior performance when compared to the 4×, 8×, and 10× AR networks.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <h2 class="title is-4">Noise level shift adaptation</h2>
      <img src="static/images/noise_level_shift.png" alt="MY ALT TEXT" width="600" />
      <h2 style="font-size: 16px; max-width: 600px;" class="subtitle has-text-justified">
        Fig. 7: Visual results of models trained at specific noise levels and our modulated network under measurement level. The last row shows the $20\times$amplified residual of the reconstructed image under no measurement noise.
      </h2>
    </div>
  </div>
</div></center>
</div>
</section>
<!-- End image carousel -->








<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="static/pdfs/paper.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{yismaw2023domain,
      title={Domain Expansion via Network Adaptation for Solving Inverse Problems},
      author={Nebiyou Yismaw and Ulugbek S. Kamilov and M. Salman Asif},
      year={2023},
      eprint={2310.06235},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
